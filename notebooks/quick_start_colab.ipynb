{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "<a href=\"https://colab.research.google.com/github/cghart/batch_invariant_ops/blob/main/notebooks/quick_start_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Batch Invariant Operations - Quick Start\n",
    "\n",
    "**⚡ One-click setup and test for batch-invariant operations**\n",
    "\n",
    "**Important**: Make sure you're using a GPU runtime:\n",
    "- Runtime → Change runtime type → Hardware accelerator: GPU\n",
    "\n",
    "This notebook will automatically:\n",
    "1. ✅ Check GPU availability\n",
    "2. 📦 Install the library\n",
    "3. 🧪 Run tests\n",
    "4. 📊 Show results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "one_click_setup"
   },
   "outputs": [],
   "source": "# 🚀 ONE-CLICK SETUP AND TEST\n# Run this cell to set everything up and test the library!\n\nimport subprocess\nimport sys\n\n# Check GPU\nprint(\"🔍 Checking GPU availability...\")\ntry:\n    gpu_info = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], \n                             capture_output=True, text=True)\n    gpu_name, gpu_memory = gpu_info.stdout.strip().split(', ')\n    print(f\"✅ GPU detected: {gpu_name} ({int(gpu_memory)/1024:.1f} GB)\")\nexcept:\n    print(\"❌ No GPU detected! Please enable GPU runtime.\")\n    print(\"   Go to: Runtime → Change runtime type → Hardware accelerator: GPU\")\n    sys.exit(1)\n\n# Install dependencies and library\nprint(\"\\n📦 Installing batch-invariant-ops...\")\n!git clone https://github.com/cghart/batch_invariant_ops.git\n%cd batch_invariant_ops\n!pip install triton -q\n!pip install -e . -q\n\n# Quick import test\nprint(\"\\n🔧 Testing imports...\")\nimport torch\nfrom batch_invariant_ops import set_batch_invariant_mode\nprint(f\"✅ PyTorch {torch.__version__} with CUDA {torch.version.cuda}\")\nprint(f\"✅ Library imported successfully\")\n\n# Run the main test\nprint(\"\\n🧪 Running batch invariance test...\")\n!python test_batch_invariance.py\n\nprint(\"\\n🎉 Setup complete! Your library is ready to use.\")\nprint(\"\\n📖 Next steps:\")\nprint(\"   • Open the full demo notebook for detailed examples\")\nprint(\"   • Integrate with your own models using set_batch_invariant_mode()\")\nprint(\"   • Check out deterministic_vllm_inference.py for vLLM usage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_example"
   },
   "source": [
    "## ⚡ Quick Example\n",
    "\n",
    "Here's a minimal example showing batch invariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example_code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from batch_invariant_ops import set_batch_invariant_mode\n",
    "\n",
    "# Set up test data\n",
    "torch.set_default_device('cuda')\n",
    "a = torch.randn(100, 200)\n",
    "b = torch.randn(200, 300)\n",
    "\n",
    "print(\"🔬 Comparing batch invariance:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test standard PyTorch\n",
    "with set_batch_invariant_mode(False):\n",
    "    result1 = torch.mm(a[:1], b)   # Single row\n",
    "    result2 = torch.mm(a, b)[:1]   # Full batch, then slice\n",
    "    diff_standard = (result1 - result2).abs().max().item()\n",
    "\n",
    "print(f\"Standard PyTorch difference: {diff_standard:.2e}\")\n",
    "\n",
    "# Test batch-invariant operations\n",
    "with set_batch_invariant_mode(True):\n",
    "    result3 = torch.mm(a[:1], b)   # Single row\n",
    "    result4 = torch.mm(a, b)[:1]   # Full batch, then slice\n",
    "    diff_invariant = (result3 - result4).abs().max().item()\n",
    "\n",
    "print(f\"Batch-invariant difference:  {diff_invariant:.2e}\")\n",
    "print(\"\\n✅ Batch-invariant ops ensure identical results!\" if diff_invariant == 0 else \"❌ Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage"
   },
   "source": [
    "## 🔧 Basic Usage\n",
    "\n",
    "Use the library in your code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usage_example"
   },
   "outputs": [],
   "source": [
    "# Example: Making your model deterministic\n",
    "from batch_invariant_ops import set_batch_invariant_mode\n",
    "\n",
    "# Your model and data\n",
    "# model = YourModel()\n",
    "# input_data = your_data\n",
    "\n",
    "# Enable batch-invariant mode for deterministic inference\n",
    "# with set_batch_invariant_mode(True):\n",
    "#     output = model(input_data)\n",
    "\n",
    "print(\"💡 Copy this pattern into your own code!\")\n",
    "print(\"\\n📚 For more examples, check out:\")\n",
    "print(\"   • The full demo notebook\")\n",
    "print(\"   • deterministic_vllm_inference.py\")\n",
    "print(\"   • GitHub repository documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resources"
   },
   "source": "## 📚 Resources\n\n- **📓 Full Demo**: [Open the complete notebook](./batch_invariant_ops_colab.ipynb) for detailed examples and benchmarks\n- **🔗 GitHub**: [Repository](https://github.com/cghart/batch_invariant_ops) with source code and documentation\n- **📝 Blog Post**: [Defeating Nondeterminism in LLM Inference](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/)\n\n## 🚀 Scaling Up\n\nWhen you need more than Colab's free tier:\n- **[Paperspace Gradient](https://gradient.paperspace.com)**: Better GPUs, longer sessions\n- **[Vast.ai](https://vast.ai)**: Cost-effective GPU rentals\n- **[Lambda Labs](https://lambdalabs.com)**: Professional ML infrastructure\n\n---\n\n**🎉 You're all set! Happy deterministic computing!**"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}