{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "<a href=\"https://colab.research.google.com/github/cghart/batch_invariant_ops/blob/main/notebooks/batch_invariant_ops_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Batch Invariant Operations - Google Colab Demo\n",
    "\n",
    "This notebook demonstrates the batch-invariant-ops library, which provides deterministic GPU operations that produce identical results regardless of batch size.\n",
    "\n",
    "**Important**: Make sure you're using a GPU runtime:\n",
    "- Go to Runtime → Change runtime type → Hardware accelerator: GPU\n",
    "- Recommended: T4 GPU (free) or better (Colab Pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 🚀 Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specifications\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if nvidia-smi is available\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    print(\"🎯 GPU Information:\")\n",
    "    print(result.stdout)\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ No GPU detected! Please enable GPU runtime:\")\n",
    "    print(\"   Runtime → Change runtime type → Hardware accelerator: GPU\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pytorch_check"
   },
   "outputs": [],
   "source": [
    "# Check PyTorch and CUDA\n",
    "import torch\n",
    "print(f\"🐍 Python version: {sys.version}\")\n",
    "print(f\"🔥 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🚀 CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"🔧 CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"🏗️ CuDNN version: {torch.backends.cudnn.version()}\")\n",
    "else:\n",
    "    print(\"❌ CUDA not available! Please check GPU runtime setting.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## 📦 Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": "# Clone the repository\n!git clone https://github.com/cghart/batch_invariant_ops.git\n%cd batch_invariant_ops"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install Triton (required for kernels)\n",
    "!pip install triton -q\n",
    "\n",
    "# Install the library in development mode\n",
    "!pip install -e . -q\n",
    "\n",
    "# Install visualization dependencies\n",
    "!pip install matplotlib seaborn pandas -q\n",
    "\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## 🔍 Environment Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_test"
   },
   "outputs": [],
   "source": [
    "# Test imports\n",
    "try:\n",
    "    import batch_invariant_ops\n",
    "    from batch_invariant_ops import (\n",
    "        set_batch_invariant_mode,\n",
    "        is_batch_invariant_mode_enabled,\n",
    "        matmul_persistent,\n",
    "        log_softmax,\n",
    "        mean_dim\n",
    "    )\n",
    "    print(f\"✅ Library imported successfully! Version: {batch_invariant_ops.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick_test"
   },
   "outputs": [],
   "source": [
    "# Quick functionality test\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "# Test batch-invariant mode enablement\n",
    "print(f\"📊 Initial batch-invariant mode: {is_batch_invariant_mode_enabled()}\")\n",
    "\n",
    "with set_batch_invariant_mode(True):\n",
    "    print(f\"📊 Batch-invariant mode enabled: {is_batch_invariant_mode_enabled()}\")\n",
    "    \n",
    "    # Quick test with small tensors\n",
    "    a = torch.randn(4, 4)\n",
    "    b = torch.randn(4, 4)\n",
    "    c = torch.mm(a, b)\n",
    "    print(f\"✅ Test matrix multiplication successful! Shape: {c.shape}\")\n",
    "\n",
    "print(f\"📊 Final batch-invariant mode: {is_batch_invariant_mode_enabled()}\")\n",
    "print(\"🎉 Environment validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo"
   },
   "source": [
    "## 🎯 Batch Invariance Demo\n",
    "\n",
    "This section demonstrates the core concept: standard PyTorch operations can produce different results depending on batch size due to floating-point precision and execution order, while batch-invariant operations produce identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_setup"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the demo data\n",
    "torch.set_default_device('cuda')\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "\n",
    "def test_batch_invariance_detailed():\n",
    "    \"\"\"Detailed test showing batch variance in standard PyTorch vs invariance with library\"\"\"\n",
    "    B, D = 1024, 2048  # Batch size, feature dimension\n",
    "    \n",
    "    # Create test data\n",
    "    a = torch.linspace(-100, 100, B*D, dtype=torch.float32).reshape(B, D)\n",
    "    b = torch.linspace(-50, 50, D*D, dtype=torch.float32).reshape(D, D)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Test different batch sizes\n",
    "    test_sizes = [1, 8, 32, 128, 512, 1024]\n",
    "    \n",
    "    for batch_size in test_sizes:\n",
    "        if batch_size > B:\n",
    "            continue\n",
    "            \n",
    "        # Method 1: Process batch_size rows directly\n",
    "        out1 = torch.mm(a[:batch_size], b)\n",
    "        \n",
    "        # Method 2: Process full batch, then slice\n",
    "        out2 = torch.mm(a, b)[:batch_size]\n",
    "        \n",
    "        # Calculate difference\n",
    "        diff = (out1 - out2).abs().max().item()\n",
    "        \n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'standard_pytorch_diff': diff,\n",
    "            'is_identical': diff == 0.0\n",
    "        })\n",
    "        \n",
    "        print(f\"Batch size {batch_size:4d}: Max difference = {diff:.2e} | Identical: {diff == 0.0}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"🔬 Testing standard PyTorch (may show differences):\")\n",
    "with set_batch_invariant_mode(False):\n",
    "    standard_results = test_batch_invariance_detailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_batch_invariant"
   },
   "outputs": [],
   "source": [
    "print(\"\\n🎯 Testing with batch-invariant operations:\")\n",
    "with set_batch_invariant_mode(True):\n",
    "    invariant_results = test_batch_invariance_detailed()\n",
    "\n",
    "# Combine results for comparison\n",
    "combined = pd.merge(standard_results, invariant_results, on='batch_size', suffixes=('_standard', '_invariant'))\n",
    "print(\"\\n📊 Summary:\")\n",
    "print(combined[['batch_size', 'standard_pytorch_diff', 'is_identical_standard', 'is_identical_invariant']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization"
   },
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Differences in standard PyTorch\n",
    "ax1.semilogy(combined['batch_size'], combined['standard_pytorch_diff'] + 1e-20, 'o-', color='red', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Max Difference (log scale)')\n",
    "ax1.set_title('Standard PyTorch: Batch Size Effects')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(1e-20, 1e-5)\n",
    "\n",
    "# Plot 2: Consistency comparison\n",
    "consistency_data = {\n",
    "    'Standard PyTorch': combined['is_identical_standard'].sum(),\n",
    "    'Batch-Invariant Ops': combined['is_identical_invariant'].sum()\n",
    "}\n",
    "bars = ax2.bar(consistency_data.keys(), consistency_data.values(), color=['red', 'green'], alpha=0.7)\n",
    "ax2.set_ylabel('Number of Identical Results')\n",
    "ax2.set_title('Consistency Across Batch Sizes')\n",
    "ax2.set_ylim(0, len(combined))\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, consistency_data.values()):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{value}/{len(combined)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Results: Batch-invariant operations are consistent across {invariant_results['is_identical'].sum()}/{len(invariant_results)} batch sizes\")\n",
    "print(f\"❌ Standard PyTorch is consistent across {standard_results['is_identical'].sum()}/{len(standard_results)} batch sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comprehensive_test"
   },
   "source": [
    "## 🧪 Comprehensive Testing\n",
    "\n",
    "Run the full test suite included with the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_tests"
   },
   "outputs": [],
   "source": [
    "# Run the official test suite\n",
    "print(\"🧪 Running official test suite...\")\n",
    "!python test_batch_invariance.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "operations_demo"
   },
   "source": [
    "## ⚙️ Supported Operations Demo\n",
    "\n",
    "Test each supported operation individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_matmul"
   },
   "outputs": [],
   "source": [
    "# Test matrix multiplication (mm and addmm)\n",
    "print(\"🧮 Testing Matrix Operations:\")\n",
    "\n",
    "def test_operation(op_name, operation, *args):\n",
    "    \"\"\"Test an operation with and without batch-invariant mode\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Standard PyTorch\n",
    "    with set_batch_invariant_mode(False):\n",
    "        result_std = operation(*args)\n",
    "    \n",
    "    # Batch-invariant mode\n",
    "    with set_batch_invariant_mode(True):\n",
    "        result_inv = operation(*args)\n",
    "    \n",
    "    # Check if results are similar (they should be for single operations)\n",
    "    diff = (result_std - result_inv).abs().max().item()\n",
    "    print(f\"  {op_name}: difference = {diff:.2e}\")\n",
    "    return result_std, result_inv, diff\n",
    "\n",
    "# Matrix multiplication\n",
    "a = torch.randn(128, 256, device='cuda')\n",
    "b = torch.randn(256, 512, device='cuda')\n",
    "test_operation(\"torch.mm\", torch.mm, a, b)\n",
    "\n",
    "# Matrix multiplication with bias\n",
    "bias = torch.randn(512, device='cuda')\n",
    "test_operation(\"torch.addmm\", torch.addmm, bias, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_log_softmax"
   },
   "outputs": [],
   "source": [
    "# Test log softmax\n",
    "print(\"\\n📊 Testing Log Softmax:\")\n",
    "x = torch.randn(64, 1000, device='cuda')\n",
    "test_operation(\"torch.log_softmax\", torch.log_softmax, x, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_mean"
   },
   "outputs": [],
   "source": [
    "# Test mean operations\n",
    "print(\"\\n📈 Testing Mean Operations:\")\n",
    "x = torch.randn(32, 64, 128, device='cuda')\n",
    "\n",
    "# Mean along single dimension\n",
    "test_operation(\"torch.mean(dim=1)\", torch.mean, x, 1)\n",
    "\n",
    "# Mean along multiple dimensions\n",
    "test_operation(\"torch.mean(dim=[1,2])\", torch.mean, x, [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "benchmark"
   },
   "source": [
    "## 🏎️ Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_code"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_operation(name, operation, *args, warmup=5, iterations=50):\n",
    "    \"\"\"Benchmark an operation with both modes\"\"\"\n",
    "    \n",
    "    def time_operation(mode_enabled):\n",
    "        with set_batch_invariant_mode(mode_enabled):\n",
    "            # Warmup\n",
    "            for _ in range(warmup):\n",
    "                _ = operation(*args)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Timing\n",
    "            start = time.time()\n",
    "            for _ in range(iterations):\n",
    "                _ = operation(*args)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            \n",
    "            return (end - start) / iterations * 1000  # ms per operation\n",
    "    \n",
    "    time_standard = time_operation(False)\n",
    "    time_invariant = time_operation(True)\n",
    "    \n",
    "    speedup = time_standard / time_invariant\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Standard PyTorch: {time_standard:.3f} ms\")\n",
    "    print(f\"  Batch-Invariant:  {time_invariant:.3f} ms\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x\\n\")\n",
    "    \n",
    "    return time_standard, time_invariant, speedup\n",
    "\n",
    "print(\"🏎️ Performance Benchmarks:\")\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "# Large matrix multiplication\n",
    "a_large = torch.randn(2048, 4096, device='cuda')\n",
    "b_large = torch.randn(4096, 2048, device='cuda')\n",
    "benchmark_operation(\"Large Matrix Multiplication (2048x4096 @ 4096x2048)\", torch.mm, a_large, b_large)\n",
    "\n",
    "# Log softmax\n",
    "x_large = torch.randn(1024, 32000, device='cuda')\n",
    "benchmark_operation(\"Log Softmax (1024x32000)\", torch.log_softmax, x_large, -1)\n",
    "\n",
    "# Mean operation\n",
    "x_mean = torch.randn(256, 512, 1024, device='cuda')\n",
    "benchmark_operation(\"Mean (256x512x1024, dim=1)\", torch.mean, x_mean, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "memory"
   },
   "source": [
    "## 💾 Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory_analysis"
   },
   "outputs": [],
   "source": [
    "def measure_memory_usage(operation, *args, mode_enabled=True):\n",
    "    \"\"\"Measure peak GPU memory usage during operation\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    with set_batch_invariant_mode(mode_enabled):\n",
    "        result = operation(*args)\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB\n",
    "    return peak_memory\n",
    "\n",
    "print(\"💾 Memory Usage Comparison:\")\n",
    "print(\"===========================\\n\")\n",
    "\n",
    "# Test with different sized operations\n",
    "test_cases = [\n",
    "    (\"Medium MatMul (1024x2048 @ 2048x1024)\", torch.mm, torch.randn(1024, 2048, device='cuda'), torch.randn(2048, 1024, device='cuda')),\n",
    "    (\"Large LogSoftmax (512x16000)\", torch.log_softmax, torch.randn(512, 16000, device='cuda'), -1),\n",
    "]\n",
    "\n",
    "for name, op, *args in test_cases:\n",
    "    mem_standard = measure_memory_usage(op, *args, mode_enabled=False)\n",
    "    mem_invariant = measure_memory_usage(op, *args, mode_enabled=True)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Standard PyTorch: {mem_standard:.3f} GB\")\n",
    "    print(f\"  Batch-Invariant:  {mem_invariant:.3f} GB\")\n",
    "    print(f\"  Difference: {mem_invariant - mem_standard:+.3f} GB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export"
   },
   "source": [
    "## 📁 Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_results"
   },
   "outputs": [],
   "source": [
    "# Create a summary report\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# System information\n",
    "system_info = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'gpu_name': torch.cuda.get_device_name(0),\n",
    "    'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_version': torch.version.cuda,\n",
    "    'library_version': batch_invariant_ops.__version__,\n",
    "    'platform': 'Google Colab'\n",
    "}\n",
    "\n",
    "# Test results summary\n",
    "test_summary = {\n",
    "    'batch_invariance_test': {\n",
    "        'standard_pytorch_consistent_results': int(standard_results['is_identical'].sum()),\n",
    "        'batch_invariant_consistent_results': int(invariant_results['is_identical'].sum()),\n",
    "        'total_test_cases': len(standard_results)\n",
    "    }\n",
    "}\n",
    "\n",
    "report = {\n",
    "    'system_info': system_info,\n",
    "    'test_summary': test_summary\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('colab_test_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# Save detailed results as CSV\n",
    "combined.to_csv('batch_invariance_detailed_results.csv', index=False)\n",
    "\n",
    "print(\"📁 Results exported:\")\n",
    "print(\"  - colab_test_report.json (summary)\")\n",
    "print(\"  - batch_invariance_detailed_results.csv (detailed data)\")\n",
    "print(\"\\n📋 Summary:\")\n",
    "print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "Congratulations! You've successfully tested the batch-invariant-ops library. Here are some next steps:\n",
    "\n",
    "### 🔧 **Integration into Your Projects**\n",
    "```python\n",
    "from batch_invariant_ops import set_batch_invariant_mode\n",
    "\n",
    "# Enable for deterministic inference\n",
    "with set_batch_invariant_mode(True):\n",
    "    model_output = your_model(input_data)\n",
    "```\n",
    "\n",
    "### 📚 **Learn More**\n",
    "- Read the full documentation in the repository\n",
    "- Check out `deterministic_vllm_inference.py` for vLLM integration\n",
    "- Explore the source code in `batch_invariant_ops/batch_invariant_ops.py`\n",
    "\n",
    "### 🎯 **When You Need More Resources**\n",
    "- **Longer experiments**: Consider [Paperspace Gradient](https://gradient.paperspace.com) or [Lambda Labs](https://lambdalabs.com)\n",
    "- **Cost optimization**: Try [Vast.ai](https://vast.ai) for cheaper GPU rentals\n",
    "- **Production deployment**: Use cloud providers like AWS, GCP, or Azure\n",
    "\n",
    "### 💡 **Contribute**\n",
    "- Report issues on the GitHub repository\n",
    "- Share your use cases and benchmarks\n",
    "- Contribute improvements or new operations\n",
    "\n",
    "---\n",
    "\n",
    "**Happy deterministic computing! 🎉**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}